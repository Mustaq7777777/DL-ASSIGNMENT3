{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmQT8acC1RzgsThgprYMHl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustaq7777777/DL-ASSIGNMENT3/blob/main/DL_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Imports"
      ],
      "metadata": {
        "id": "DUXqOGN6GO0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# For reproducibility\n",
        "def seed_everything(seed=42):\n",
        "    \"\"\"Set random seed for all major libraries\"\"\"\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_everything(42)\n",
        "\n",
        "# Device selection: CPU or GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "KAJ4FenEGJkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and Extracting the Dakshina Dataset"
      ],
      "metadata": {
        "id": "CW37qPkIGeIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Dakshina dataset\n",
        "!yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n",
        "\n",
        "# Extract the downloaded tar file\n",
        "!yes | tar xopf dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "i7fxSiDbGkHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading and Processing Functions"
      ],
      "metadata": {
        "id": "nGb1w7TGGo9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_tsv(file_path):\n",
        "    \"\"\"Read a tab-separated file with source and target text\"\"\"\n",
        "    eng_words = []\n",
        "    tel_words = []\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        for ln in f:\n",
        "            parts = ln.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                tel_words.append(parts[0])  # Dakshina format has target first\n",
        "                eng_words.append(parts[1])  # Source (English) second\n",
        "    return eng_words, tel_words\n",
        "\n",
        "def load_dakshina_data(language='tel', base_path=None):\n",
        "    \"\"\"Load transliteration data from Dakshina TSV files\"\"\"\n",
        "    if base_path is None:\n",
        "        # Default path structure for Dakshina\n",
        "        base_path = os.path.join(\n",
        "            '/kaggle/working/dakshina_dataset_v1.0',\n",
        "            language, 'lexicons'\n",
        "        )\n",
        "\n",
        "    # Paths to data files\n",
        "    train_file = os.path.join(base_path, f\"{language}.translit.sampled.train.tsv\")\n",
        "    valid_file = os.path.join(base_path, f\"{language}.translit.sampled.dev.tsv\")\n",
        "    test_file = os.path.join(base_path, f\"{language}.translit.sampled.test.tsv\")\n",
        "\n",
        "    # Load data\n",
        "    eng_list_train, tel_list_train = read_tsv(train_file)\n",
        "    eng_list_valid, tel_list_valid = read_tsv(valid_file)\n",
        "    eng_list_test, tel_list_test = read_tsv(test_file)\n",
        "\n",
        "    # Build vocabularies\n",
        "    eng_vocab = []\n",
        "    tel_vocab = []\n",
        "    max_eng_len = -1\n",
        "    max_tel_len = -1\n",
        "    max_eng_word = \"\"\n",
        "    max_tel_word = \"\"\n",
        "\n",
        "    # Process training data for vocabulary\n",
        "    for word in eng_list_train:\n",
        "        max_eng_len = max(max_eng_len, len(word))\n",
        "        if max_eng_len == len(word):\n",
        "            max_eng_word = word\n",
        "        for letter in word:\n",
        "            eng_vocab.append(letter)\n",
        "    eng_vocab = list(set(eng_vocab))\n",
        "    eng_vocab.sort()\n",
        "\n",
        "    for word in tel_list_train:\n",
        "        max_tel_len = max(max_tel_len, len(word))\n",
        "        if max_tel_len == len(word):\n",
        "            max_tel_word = word\n",
        "        for letter in word:\n",
        "            tel_vocab.append(letter)\n",
        "    tel_vocab = list(set(tel_vocab))\n",
        "    tel_vocab.sort()\n",
        "\n",
        "    # Update max lengths from validation and test sets\n",
        "    for word in eng_list_valid:\n",
        "        max_eng_len = max(max_eng_len, len(word))\n",
        "    for word in eng_list_test:\n",
        "        max_eng_len = max(max_eng_len, len(word))\n",
        "    for word in tel_list_test:\n",
        "        max_tel_len = max(max_tel_len, len(word))\n",
        "    for word in tel_list_valid:\n",
        "        max_tel_len = max(max_tel_len, len(word))\n",
        "\n",
        "    #printing the values to know about data\n",
        "\n",
        "    print(f\"English vocabulary size: {len(eng_vocab)}\")\n",
        "    print(f\"Target language vocabulary size: {len(tel_vocab)}\")\n",
        "    print(f\"Max English length: {max_eng_len}\")\n",
        "    print(f\"Max target language length: {max_tel_len}\")\n",
        "    print(f\"Training examples: {len(eng_list_train)}\")\n",
        "\n",
        "    return (eng_list_train, tel_list_train, eng_list_valid, tel_list_valid,\n",
        "            eng_list_test, tel_list_test, eng_vocab, tel_vocab,\n",
        "            max_eng_len, max_tel_len)"
      ],
      "metadata": {
        "id": "iMSCOBPhGuMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Vectorization"
      ],
      "metadata": {
        "id": "wUZYrM5KG-vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_vector(language, word, eng_vocab, tel_vocab, max_eng_len, max_tel_len):\n",
        "    \"\"\"Convert a word to its vectorial representation\"\"\"\n",
        "    vec = []\n",
        "    if language == \"english\":\n",
        "        # Start token\n",
        "        vec.append(len(eng_vocab) + 1)\n",
        "        # Word content\n",
        "        for letter in word:\n",
        "            for albt in range(len(eng_vocab)):\n",
        "                if eng_vocab[albt] == letter:\n",
        "                    vec.append(albt + 1)\n",
        "        # Padding\n",
        "        while len(vec) < (max_eng_len + 1):\n",
        "            vec.append(0)\n",
        "        # End token\n",
        "        vec.append(0)\n",
        "    else:\n",
        "        # Start token\n",
        "        vec.append(len(tel_vocab) + 1)\n",
        "        # Word content\n",
        "        for letter in word:\n",
        "            for albt in range(len(tel_vocab)):\n",
        "                if tel_vocab[albt] == letter:\n",
        "                    vec.append(albt + 1)\n",
        "        # Padding\n",
        "        while len(vec) < (max_tel_len + 1):\n",
        "            vec.append(0)\n",
        "        # End token\n",
        "        vec.append(0)\n",
        "    return vec\n",
        "\n",
        "def prepare_matrices(eng_list, tel_list, eng_vocab, tel_vocab, max_eng_len, max_tel_len):\n",
        "    \"\"\"Create tensor matrices from word lists\"\"\"\n",
        "    eng_matrix = []\n",
        "    tel_matrix = []\n",
        "\n",
        "    for word in eng_list:\n",
        "        eng_matrix.append(word_to_vector(\"english\", word, eng_vocab, tel_vocab, max_eng_len, max_tel_len))\n",
        "\n",
        "    for word in tel_list:\n",
        "        tel_matrix.append(word_to_vector(\"telugu\", word, eng_vocab, tel_vocab, max_eng_len, max_tel_len))\n",
        "\n",
        "    return torch.tensor(eng_matrix), torch.tensor(tel_matrix)"
      ],
      "metadata": {
        "id": "6tbXY6_5HDXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "metadata": {
        "id": "QDq3O1WlHaG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = load_dakshina_data('tel')\n",
        "(eng_list_train, tel_list_train, eng_list_valid, tel_list_valid,\n",
        " eng_list_test, tel_list_test, eng_vocab, tel_vocab,\n",
        " max_eng_len, max_tel_len) = data\n",
        "\n",
        "# Prepare matrices\n",
        "eng_matrix_train, tel_matrix_train = prepare_matrices(\n",
        "    eng_list_train, tel_list_train, eng_vocab, tel_vocab, max_eng_len, max_tel_len\n",
        ")\n",
        "\n",
        "eng_matrix_valid, tel_matrix_valid = prepare_matrices(\n",
        "    eng_list_valid, tel_list_valid, eng_vocab, tel_vocab, max_eng_len, max_tel_len\n",
        ")\n",
        "\n",
        "eng_matrix_test, tel_matrix_test = prepare_matrices(\n",
        "    eng_list_test, tel_list_test, eng_vocab, tel_vocab, max_eng_len, max_tel_len\n",
        ")\n",
        "\n",
        "print(f\"Training matrices shape: English {eng_matrix_train.shape}, Telugu {tel_matrix_train.shape}\")\n",
        "print(f\"Validation matrices shape: English {eng_matrix_valid.shape}, Telugu {tel_matrix_valid.shape}\")\n",
        "print(f\"Test matrices shape: English {eng_matrix_test.shape}, Telugu {tel_matrix_test.shape}\")"
      ],
      "metadata": {
        "id": "rZD6ynjXHcIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "jiigX8xiHt60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, enc_layers, hidden_size,\n",
        "                 cell_type, bi_directional_bit, dropout, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.enc_layers = enc_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.bi_directional_bit = bi_directional_bit\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize RNN based on cell type\n",
        "        if cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, enc_layers,\n",
        "                             dropout=dropout, bidirectional=bi_directional_bit)\n",
        "        elif cell_type == \"GRU\":\n",
        "            self.gru = nn.GRU(embedding_size, hidden_size, enc_layers,\n",
        "                             dropout=dropout, bidirectional=bi_directional_bit)\n",
        "        else:  # LSTM\n",
        "            self.lstm = nn.LSTM(embedding_size, hidden_size, enc_layers,\n",
        "                               dropout=dropout, bidirectional=bi_directional_bit)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        \"\"\"Forward pass through the encoder\"\"\"\n",
        "        # Apply embedding and reshape\n",
        "        embedding = self.embedding(x).view(-1, self.batch_size, self.embedding_size)\n",
        "\n",
        "        # Pass through the appropriate RNN type\n",
        "        if self.cell_type == \"RNN\":\n",
        "            output, hidden = self.rnn(embedding, hidden)\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            output, hidden = self.gru(embedding, hidden)\n",
        "        else:  # LSTM\n",
        "            output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
        "            return output, hidden, cell\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initialize_hidden(self):\n",
        "        \"\"\"Initialize hidden state tensor\"\"\"\n",
        "        if self.bi_directional_bit:\n",
        "            return torch.zeros(2 * self.enc_layers, self.batch_size,\n",
        "                               self.hidden_size, device=device)\n",
        "        return torch.zeros(self.enc_layers, self.batch_size,\n",
        "                           self.hidden_size, device=device)\n",
        "\n",
        "    def initialize_cell(self):\n",
        "        \"\"\"Initialize cell state tensor (for LSTM)\"\"\"\n",
        "        if self.bi_directional_bit:\n",
        "            return torch.zeros(2 * self.enc_layers, self.batch_size,\n",
        "                               self.hidden_size, device=device)\n",
        "        return torch.zeros(self.enc_layers, self.batch_size,\n",
        "                           self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "DRfk2zZLHuMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau Attention Mechanism"
      ],
      "metadata": {
        "id": "EwivfYuvH1Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, dec_hid_dim]\n",
        "        # encoder_outputs: [src_len, batch_size, enc_hid_dim]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # Repeat hidden for src_len times\n",
        "        # [batch_size, dec_hid_dim] -> [batch_size, src_len, dec_hid_dim]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        # Transpose encoder outputs for attention calculation\n",
        "        # [src_len, batch_size, enc_hid_dim] -> [batch_size, src_len, enc_hid_dim]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        # [batch_size, src_len, enc_hid_dim + dec_hid_dim] -> [batch_size, src_len, dec_hid_dim]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        # [batch_size, src_len, dec_hid_dim] -> [batch_size, src_len, 1]\n",
        "        attention = self.v(energy)\n",
        "\n",
        "        # [batch_size, src_len, 1] -> [batch_size, src_len]\n",
        "        attention = attention.squeeze(2)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        # [batch_size, src_len]\n",
        "        return func.softmax(attention, dim=1)"
      ],
      "metadata": {
        "id": "hGxptHPQH85F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder (without attention)"
      ],
      "metadata": {
        "id": "rGGHE1z1IFRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, dec_layers,\n",
        "                 dropout, cell_type, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dec_layers = dec_layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.cell_type = cell_type\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        # Initialize RNN based on cell type\n",
        "        if cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, dec_layers, dropout=dropout)\n",
        "        elif cell_type == \"GRU\":\n",
        "            self.gru = nn.GRU(embedding_size, hidden_size, dec_layers, dropout=dropout)\n",
        "        else:  # LSTM\n",
        "            self.lstm = nn.LSTM(embedding_size, hidden_size, dec_layers, dropout=dropout)\n",
        "\n",
        "        # Output projection\n",
        "        self.fully_conc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, prev_output, prev_hidden, cell=0):\n",
        "        \"\"\"Forward pass through the decoder\"\"\"\n",
        "        # Reshape input token and apply embedding\n",
        "        x = x.unsqueeze(0).int()\n",
        "        embedding = self.embedding(x)\n",
        "        embedding = self.dropout(embedding)\n",
        "\n",
        "        # Pass through the appropriate RNN type\n",
        "        if self.cell_type == \"RNN\":\n",
        "            outputs, hidden = self.rnn(embedding, prev_hidden)\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            outputs, hidden = self.gru(embedding, prev_hidden)\n",
        "        else:  # LSTM\n",
        "            outputs, (hidden, cell) = self.lstm(embedding, (prev_hidden, cell))\n",
        "\n",
        "        # Project to vocabulary size\n",
        "        pred = self.fully_conc(outputs)\n",
        "        pred = pred.squeeze(0)  # Remove sequence dimension\n",
        "\n",
        "        if self.cell_type == \"GRU\" or self.cell_type == \"RNN\":\n",
        "            return pred, hidden\n",
        "\n",
        "        return pred, hidden, cell"
      ],
      "metadata": {
        "id": "O9jkmF96IJqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder(with Attention)"
      ],
      "metadata": {
        "id": "kKlKC5FfuYoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size,\n",
        "                 cell_type, dec_layers, dropout, bi_directional_bit):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.cell_type = cell_type\n",
        "        self.dec_layers = dec_layers\n",
        "        self.bi_directional_bit = bi_directional_bit\n",
        "        self.embedding_size = embedding_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = BahdanauAttention(hidden_size, hidden_size)\n",
        "\n",
        "        # RNN input dimension (embedding + context)\n",
        "        self.rnn_input_dim = embedding_size + hidden_size\n",
        "\n",
        "        # Initialize RNN based on cell type\n",
        "        if cell_type == \"LSTM\":\n",
        "            self.lstm = nn.LSTM(self.rnn_input_dim, hidden_size, dec_layers, dropout=dropout)\n",
        "        elif cell_type == \"GRU\":\n",
        "            self.gru = nn.GRU(self.rnn_input_dim, hidden_size, dec_layers, dropout=dropout)\n",
        "        else:  # RNN\n",
        "            self.rnn = nn.RNN(self.rnn_input_dim, hidden_size, dec_layers, dropout=dropout)\n",
        "\n",
        "        # Output projection (combines hidden state, context vector, and embedding)\n",
        "        self.fully_conc = nn.Linear(hidden_size + hidden_size + embedding_size, output_size)\n",
        "\n",
        "    def forward(self, x, encoder_outputs, prev_hidden, cell=0):\n",
        "        \"\"\"Forward pass with attention mechanism\"\"\"\n",
        "        # Get the last layer's hidden state\n",
        "        if self.cell_type == 'LSTM':\n",
        "            attention_hidden = prev_hidden[0][-1]\n",
        "        else:\n",
        "            attention_hidden = prev_hidden[-1]\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.attention(attention_hidden, encoder_outputs)\n",
        "\n",
        "        # Create context vector by applying attention weights to encoder outputs\n",
        "        # [batch_size, src_len] -> [batch_size, 1, src_len]\n",
        "        attn_weights = attn_weights.unsqueeze(1)\n",
        "\n",
        "        # [src_len, batch_size, enc_hid_dim] -> [batch_size, src_len, enc_hid_dim]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        # [batch_size, 1, src_len] x [batch_size, src_len, enc_hid_dim] -> [batch_size, 1, enc_hid_dim]\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)\n",
        "\n",
        "        # Embed input token\n",
        "        x = x.unsqueeze(0)  # Add sequence dimension\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Combine embedding and context for RNN input\n",
        "        # [1, batch_size, emb_dim], [batch_size, 1, enc_hid_dim] -> [1, batch_size, emb_dim + enc_hid_dim]\n",
        "        rnn_input = torch.cat((embedded, context.permute(1, 0, 2)), dim=2)\n",
        "\n",
        "        # Pass through the appropriate RNN type\n",
        "        if self.cell_type == \"RNN\":\n",
        "            outputs, hidden = self.rnn(rnn_input, prev_hidden)\n",
        "        elif self.cell_type == \"GRU\":\n",
        "            outputs, hidden = self.gru(rnn_input, prev_hidden)\n",
        "        else:  # LSTM\n",
        "            outputs, (hidden, cell) = self.lstm(rnn_input, (prev_hidden, cell))\n",
        "\n",
        "        # For output projection, combine hidden state, context, and embedded input\n",
        "        outputs = outputs.squeeze(0)  # Remove sequence dimension\n",
        "        embedded = embedded.squeeze(0)  # Remove sequence dimension\n",
        "        context = context.squeeze(1)   # Remove extra dimension\n",
        "\n",
        "        # Project to vocabulary size\n",
        "        pred = self.fully_conc(torch.cat((outputs, context, embedded), dim=1))\n",
        "\n",
        "        if self.cell_type == \"GRU\" or self.cell_type == \"RNN\":\n",
        "            return pred, hidden\n",
        "        else:\n",
        "            return pred, hidden, cell"
      ],
      "metadata": {
        "id": "vXUuiPGHuccv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq 2 Seq Model"
      ],
      "metadata": {
        "id": "sltUGxerugua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, decoder, encoder, cell_type, bidirectional_bit,\n",
        "                 encoder_layers, decoder_layers):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.decoder = decoder\n",
        "        self.encoder = encoder\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional_bit = bidirectional_bit\n",
        "        self.encoder_layers = encoder_layers\n",
        "        self.decoder_layers = decoder_layers\n",
        "\n",
        "    def forward(self, input_seq, target, teacher_force_ratio=0.5):\n",
        "        \"\"\"Forward pass through the sequence-to-sequence model\"\"\"\n",
        "        batch_size = input_seq.shape[1]\n",
        "        tar_seq_length = target.shape[0]\n",
        "        final_target_vocab_size = self.decoder.output_size\n",
        "\n",
        "        # Initialize outputs tensor\n",
        "        outputs = torch.zeros(tar_seq_length, batch_size,\n",
        "                             final_target_vocab_size).to(device=device)\n",
        "\n",
        "        # Initialize encoder states\n",
        "        hidden = self.encoder.initialize_hidden()\n",
        "        cell = self.encoder.initialize_cell()\n",
        "\n",
        "        # Encode input sequence\n",
        "        if self.cell_type == \"RNN\" or self.cell_type == \"GRU\":\n",
        "            encoder_output, hidden = self.encoder(input_seq, hidden, cell)\n",
        "        else:  # LSTM\n",
        "            encoder_output, hidden, cell = self.encoder(input_seq, hidden, cell)\n",
        "\n",
        "        # Handle bidirectional encoder or different layer counts\n",
        "        if self.decoder_layers != self.encoder_layers or self.bidirectional_bit:\n",
        "            if self.cell_type in [\"RNN\", \"GRU\", \"LSTM\"]:\n",
        "                # Combine bidirectional hidden states if needed\n",
        "                if self.bidirectional_bit:\n",
        "                    # Sum forward and backward directions\n",
        "                    hidden_forward = hidden[:self.encoder_layers]\n",
        "                    hidden_backward = hidden[self.encoder_layers:]\n",
        "                    hidden = hidden_forward + hidden_backward\n",
        "\n",
        "                # Match decoder layers\n",
        "                if self.decoder_layers > 1 and self.encoder_layers == 1:\n",
        "                    hidden = hidden.repeat(self.decoder_layers, 1, 1)\n",
        "\n",
        "            if self.cell_type == \"LSTM\":\n",
        "                # Also handle cell states for LSTM\n",
        "                if self.bidirectional_bit:\n",
        "                    # Sum forward and backward directions\n",
        "                    cell_forward = cell[:self.encoder_layers]\n",
        "                    cell_backward = cell[self.encoder_layers:]\n",
        "                    cell = cell_forward + cell_backward\n",
        "\n",
        "                # Match decoder layers\n",
        "                if self.decoder_layers > 1 and self.encoder_layers == 1:\n",
        "                    cell = cell.repeat(self.decoder_layers, 1, 1)\n",
        "\n",
        "        # Start with first token (SOS token)\n",
        "        x = target[0]\n",
        "\n",
        "        # Generate sequence\n",
        "        for t in range(1, tar_seq_length):\n",
        "            # Process through decoder\n",
        "            if self.cell_type == \"RNN\" or self.cell_type == \"GRU\":\n",
        "                output, hidden = self.decoder(x, encoder_output, hidden)\n",
        "            else:  # LSTM\n",
        "                output, hidden, cell = self.decoder(x, encoder_output, hidden, cell)\n",
        "\n",
        "            # Store output\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Teacher forcing: use target token with probability teacher_force_ratio\n",
        "            if random.random() < teacher_force_ratio:\n",
        "                x = target[t]\n",
        "            else:\n",
        "                # Otherwise use model's prediction\n",
        "                predicted = output.argmax(1)\n",
        "                x = predicted\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "8EKzU_ffus-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Evaluation functions"
      ],
      "metadata": {
        "id": "wJDCfyj2uuRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fun(eng_matrix, tel_matrix, batch_size, model):\n",
        "    \"\"\"Compute accuracy on a dataset\"\"\"\n",
        "    correct = 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_id in range(int(len(eng_matrix) / batch_size)):\n",
        "            # Get batch\n",
        "            inp_word = eng_matrix[batch_size * batch_id:batch_size * (batch_id + 1)].to(device=device)\n",
        "            out_word = tel_matrix[batch_size * batch_id:batch_size * (batch_id + 1)].to(device=device)\n",
        "\n",
        "            # Transpose for sequence-first format\n",
        "            inp_word = inp_word.T\n",
        "            out_word = out_word.T\n",
        "\n",
        "            # Forward pass with no teacher forcing\n",
        "            output = model.forward(inp_word, out_word, 0)\n",
        "\n",
        "            # Get predictions\n",
        "            output = nn.Softmax(dim=2)(output)\n",
        "            output = torch.argmax(output, dim=2)\n",
        "\n",
        "            # Transpose back to batch-first for comparison\n",
        "            output = output.T\n",
        "            out_word = out_word.T\n",
        "\n",
        "            # Count correct predictions (exact match of entire sequence)\n",
        "            for i in range(min(batch_size, len(inp_word))):  # Handle last batch which may be smaller\n",
        "                if torch.equal(output[i][1:], out_word[i][1:]):\n",
        "                    correct += 1\n",
        "\n",
        "    # Return accuracy percentage\n",
        "    return (correct * 100) / len(eng_matrix)\n",
        "\n",
        "def vectors_to_actual_words(model, eng_matrix, tel_matrix, batch_size, eng_vocab, tel_vocab, data_type):\n",
        "    \"\"\"Convert model predictions to readable words\"\"\"\n",
        "    results = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_id in range(int(len(eng_matrix) / batch_size)):\n",
        "            # Get batch\n",
        "            input_batch = eng_matrix[batch_id * batch_size:batch_size * (batch_id + 1)].to(device=device)\n",
        "            output_batch = tel_matrix[batch_id * batch_size:batch_size * (batch_id + 1)].to(device=device)\n",
        "\n",
        "            # Forward pass\n",
        "            model_output = model.forward(input_batch.T, output_batch.T, 0)\n",
        "            model_output = nn.Softmax(dim=2)(model_output)\n",
        "            model_output = torch.argmax(model_output, dim=2)\n",
        "            model_output = model_output.T\n",
        "\n",
        "            # Process each example\n",
        "            for idx in range(len(output_batch)):\n",
        "                res_word = output_batch[idx]\n",
        "                pred_word = model_output[idx]\n",
        "                inp_word = input_batch[idx]\n",
        "\n",
        "                # Convert to strings\n",
        "                word_res = \"\"\n",
        "                word_pred = \"\"\n",
        "                word_inp = \"\"\n",
        "\n",
        "                # Convert prediction to string\n",
        "                for i in range(len(pred_word)):\n",
        "                    if pred_word[i] > 0 and pred_word[i] < len(tel_vocab) + 1:\n",
        "                        word_pred += tel_vocab[pred_word[i] - 1]\n",
        "\n",
        "                # Convert input to string\n",
        "                for i in range(len(inp_word)):\n",
        "                    if inp_word[i] > 0 and inp_word[i] < len(eng_vocab) + 1:\n",
        "                        word_inp += eng_vocab[inp_word[i] - 1]\n",
        "\n",
        "                # Convert target to string\n",
        "                for i in range(len(res_word)):\n",
        "                    if res_word[i] > 0 and res_word[i] < len(tel_vocab) + 1:\n",
        "                        word_res += tel_vocab[res_word[i] - 1]\n",
        "\n",
        "                results.append((word_inp, word_pred, word_res))\n",
        "\n",
        "    return results\n",
        "\n",
        "def save_to_csv(results, filename):\n",
        "    \"\"\"Save results to a CSV file\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"Source,Predicted,Target\\n\")\n",
        "        for src, pred, tgt in results:\n",
        "            f.write(f\"{src},{pred},{tgt}\\n\")"
      ],
      "metadata": {
        "id": "symERFoxu15z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Evaluating the models"
      ],
      "metadata": {
        "id": "6B_PC9aBvBkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(cell_type, bi_directional_bit, embedding_size, enc_dropout,\n",
        "                     dec_dropout, enc_layers, dec_layers, hidden_size, batch_size,\n",
        "                     attention_bit, learning_rate, max_epochs, language='tel',\n",
        "                     use_wandb=False):\n",
        "    \"\"\"Train and evaluate a seq2seq model\"\"\"\n",
        "    # Initialize wandb if requested\n",
        "    if use_wandb:\n",
        "        run_name = f\"{cell_type}_{enc_layers}l_{embedding_size}e_{hidden_size}h_\" \\\n",
        "                  f\"{'attn' if attention_bit else 'no_attn'}_\" \\\n",
        "                  f\"{'bid' if bi_directional_bit else 'uni'}\"\n",
        "\n",
        "        wandb.init(\n",
        "            project=\"DL_assignment_3\",\n",
        "            name=run_name,\n",
        "            config={\n",
        "                \"cell_type\": cell_type,\n",
        "                \"bi_directional\": bi_directional_bit,\n",
        "                \"embedding_size\": embedding_size,\n",
        "                \"enc_dropout\": enc_dropout,\n",
        "                \"dec_dropout\": dec_dropout,\n",
        "                \"enc_layers\": enc_layers,\n",
        "                \"dec_layers\": dec_layers,\n",
        "                \"hidden_size\": hidden_size,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"attention\": attention_bit,\n",
        "                \"learning_rate\": learning_rate,\n",
        "                \"max_epochs\": max_epochs,\n",
        "                \"language\": language\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Get data from global variables\n",
        "    # (To keep the code structure aligned with the original)\n",
        "\n",
        "    # Model dimensions\n",
        "    enc_input_size = len(eng_vocab) + 2  # +2 for special tokens\n",
        "    dec_input_size = len(tel_vocab) + 2\n",
        "    output_size = len(tel_vocab) + 2\n",
        "\n",
        "    # Create encoder\n",
        "    encoder_section = Encoder(\n",
        "        enc_input_size, embedding_size, enc_layers, hidden_size,\n",
        "        cell_type, bi_directional_bit, enc_dropout, batch_size\n",
        "    ).to(device=device)\n",
        "\n",
        "    # Create decoder (with or without attention)\n",
        "    if attention_bit:\n",
        "        decoder_section = AttentionDecoder(\n",
        "            dec_input_size, embedding_size, hidden_size, output_size,\n",
        "            cell_type, dec_layers, dec_dropout, bi_directional_bit\n",
        "        ).to(device=device)\n",
        "    else:\n",
        "        decoder_section = Decoder(\n",
        "            dec_input_size, embedding_size, hidden_size, dec_layers,\n",
        "            dec_dropout, cell_type, output_size\n",
        "        ).to(device=device)\n",
        "\n",
        "    # Create sequence-to-sequence model\n",
        "    model = Seq2Seq(\n",
        "        decoder_section, encoder_section, cell_type,\n",
        "        bi_directional_bit, enc_layers, dec_layers\n",
        "    ).to(device=device)\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Create loss function (ignoring padding)\n",
        "    pad = len(tel_vocab) + 1\n",
        "    loss_criterion = nn.CrossEntropyLoss(ignore_index=pad)\n",
        "\n",
        "    # Main training loop\n",
        "    print(f\"Starting training for {max_epochs} epochs\")\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        print(f\"Epoch: {epoch+1}/{max_epochs}\")\n",
        "\n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        step = 0\n",
        "\n",
        "        # Training batches with progress bar\n",
        "        batch_count = int(len(eng_matrix_train) / batch_size)\n",
        "        progress_bar = tqdm(range(batch_count), desc=f\"Training {epoch+1}\")\n",
        "\n",
        "        for batch_id in progress_bar:\n",
        "            # Get batch data\n",
        "            inp_word = eng_matrix_train[batch_size * batch_id:batch_size * (batch_id + 1)].to(device=device)\n",
        "            out_word = tel_matrix_train[batch_size * batch_id:batch_size * (batch_id + 1)].to(device=device)\n",
        "\n",
        "            # Transpose for sequence-first format\n",
        "            out_word = out_word.T\n",
        "            inp_word = inp_word.T\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(inp_word, out_word)\n",
        "\n",
        "            # Calculate loss (skip first token which is SOS)\n",
        "            output = output[1:].reshape(-1, output.shape[2])\n",
        "            out_word = out_word[1:].reshape(-1)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_criterion(output, out_word)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "            step += 1\n",
        "\n",
        "        # Calculate epoch average loss\n",
        "        avg_loss = total_loss / step\n",
        "        print(f\"Total loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on train, validation, and test sets\n",
        "        train_acc = accuracy_fun(eng_matrix_train, tel_matrix_train, batch_size, model)\n",
        "        valid_acc = accuracy_fun(eng_matrix_valid, tel_matrix_valid, batch_size, model)\n",
        "        test_acc = accuracy_fun(eng_matrix_test, tel_matrix_test, batch_size, model)\n",
        "\n",
        "        print(f\"Train accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"Valid accuracy: {valid_acc:.2f}%\")\n",
        "        print(f\"Test accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        # Log to wandb if enabled\n",
        "        if use_wandb:\n",
        "            wandb.log({\n",
        "                'epoch': epoch + 1,\n",
        "                'loss': avg_loss,\n",
        "                'train_accuracy': train_acc,\n",
        "                'valid_accuracy': valid_acc,\n",
        "                'test_accuracy': test_acc\n",
        "            })\n",
        "\n",
        "    # Generate and save predictions\n",
        "    test_results = vectors_to_actual_words(\n",
        "        model, eng_matrix_test, tel_matrix_test, batch_size,\n",
        "        eng_vocab, tel_vocab, 'Test'\n",
        "    )\n",
        "    save_to_csv(test_results, f\"predictions_{cell_type}_{attention_bit}.csv\")\n",
        "\n",
        "    # Close wandb run if used\n",
        "    if use_wandb:\n",
        "        wandb.finish()\n",
        "\n",
        "    return model, (train_acc, valid_acc, test_acc), test_results"
      ],
      "metadata": {
        "id": "jTKMKLVrvF6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the models and comparing results"
      ],
      "metadata": {
        "id": "BPZSNcZavLWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for model without attention\n",
        "config_no_attention = {\n",
        "    'cell_type': 'GRU',\n",
        "    'bi_directional_bit': True,\n",
        "    'embedding_size': 256,\n",
        "    'enc_dropout': 0.2,\n",
        "    'dec_dropout': 0.2,\n",
        "    'enc_layers': 2,\n",
        "    'dec_layers': 2,\n",
        "    'hidden_size': 512,\n",
        "    'batch_size': 64,\n",
        "    'attention_bit': False,\n",
        "    'learning_rate': 0.001,\n",
        "    'max_epochs': 10,\n",
        "    'language': 'tel',\n",
        "    'use_wandb': False  # Set to True to log to wandb\n",
        "}\n",
        "\n",
        "# Train model without attention\n",
        "no_attention_model, no_attention_accuracies, no_attention_results = train_and_evaluate(**config_no_attention)\n",
        "\n",
        "print(\"\\nFinal results without attention:\")\n",
        "print(f\"Train accuracy: {no_attention_accuracies[0]:.2f}%\")\n",
        "print(f\"Valid accuracy: {no_attention_accuracies[1]:.2f}%\")\n",
        "print(f\"Test accuracy: {no_attention_accuracies[2]:.2f}%\")\n",
        "\n",
        "# Create a bar chart to compare accuracies\n",
        "labels = ['Train', 'Valid', 'Test']\n",
        "no_attention_accs = [no_attention_accuracies[0], no_attention_accuracies[1], no_attention_accuracies[2]]\n",
        "attention_accs = [attention_accuracies[0], attention_accuracies[1], attention_accuracies[2]]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rects1 = ax.bar(x - width/2, no_attention_accs, width, label='Without Attention')\n",
        "rects2 = ax.bar(x + width/2, attention_accs, width, label='With Attention')\n",
        "\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Seq2Seq Model Accuracy: With vs. Without Attention')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}%',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4ptKesPIvgBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter sweep"
      ],
      "metadata": {
        "id": "7T5i6A_Qvjgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_hyperparameter_sweep(with_attention=True):\n",
        "    \"\"\"Run hyperparameter sweep with wandb\"\"\"\n",
        "    sweep_name = 'Transliteration_with_Attention' if with_attention else 'Transliteration_without_Attention'\n",
        "\n",
        "    # Define sweep configuration\n",
        "    sweep_cfg = {\n",
        "        'method': 'bayes',  # Use Bayesian optimization\n",
        "        'name': sweep_name,\n",
        "        'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
        "        'parameters': {\n",
        "            # Model architecture\n",
        "            'emb_size': {'values': [128, 256, 512]},\n",
        "            'hidden_size': {'values': [128, 256, 512, 1024]},\n",
        "            'enc_layers': {'values': [1, 2, 3, 4]},\n",
        "            'cell': {'values': ['RNN', 'GRU', 'LSTM']},\n",
        "            'bidirectional': {'values': [True, False]},  # Bidirectional encode\n",
        "\n",
        "            # Training parameters\n",
        "            'dropout': {'values': [0.0, 0.1, 0.2, 0.3, 0.5]},\n",
        "            'lr': {'values': [1e-4, 2e-4, 5e-4, 8e-4, 1e-3]},\n",
        "            'batch_size': {'values': [32, 64, 128]},\n",
        "            'epochs': {'values': [10, 15, 20]},\n",
        "            'teacher_forcing': {'values': [0.3, 0.5, 0.7, 1.0]},  # Explicit teacher forcing\n",
        "            'optimizer': {'values': ['Adam', 'NAdam']},  # Added optimizer options\n",
        "            # Reproducibility\n",
        "            'seed': {'values': [42, 43, 44, 45, 46]},  # Different seeds for robustness\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Define the objective function for sweep\n",
        "    def sweep_objective():\n",
        "        run = wandb.init()\n",
        "        config = run.config\n",
        "\n",
        "        # Set seed for reproducibility\n",
        "        seed_everything(config.seed)\n",
        "\n",
        "        # Train model with this configuration\n",
        "        train_and_evaluate(\n",
        "            config.cell,\n",
        "            config.bidirectional,\n",
        "            config.emb_size,\n",
        "            config.dropout,\n",
        "            config.dropout,\n",
        "            config.enc_layers,\n",
        "            config.enc_layers,\n",
        "            config.hidden_size,\n",
        "            config.batch_size,\n",
        "            with_attention,\n",
        "            config.lr,\n",
        "            config.epochs\n",
        "        )\n",
        "\n",
        "    # Initialize sweep\n",
        "    entity = 'cs24m045-indian-institute-of-technology-madras'  # Replace with your wandb entity\n",
        "    project = 'DA6401-Assignment-3'\n",
        "\n",
        "    # Start sweep (uncomment to run)\n",
        "    # sweep_id = wandb.sweep(sweep_cfg, entity=entity, project=project)\n",
        "    # wandb.agent(sweep_id, function=sweep_objective, count=20)\n",
        "\n",
        "# Uncomment to run hyperparameter sweeps\n",
        "# run_hyperparameter_sweep(with_attention=True)   # For attention model\n",
        "# run_hyperparameter_sweep(with_attention=False)  # For no-attention model"
      ],
      "metadata": {
        "id": "Qi8aEd1rv5vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating heat maps"
      ],
      "metadata": {
        "id": "lLVYFXSomrEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_heatmaps.py\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.font_manager as fm\n",
        "import os\n",
        "\n",
        "def tensor_to_numpy(tensor):\n",
        "    \"\"\"Safely convert a PyTorch tensor to NumPy array\"\"\"\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "def setup_telugu_fonts():\n",
        "    \"\"\"Set up proper font handling for Telugu characters\"\"\"\n",
        "    # Common fonts that support Telugu\n",
        "    telugu_fonts = ['Nirmala UI', 'Noto Sans Telugu', 'Lohit Telugu', 'Gautami',\n",
        "                   'Telugu Sangam MN', 'Akshar Unicode', 'Vani', 'Arial Unicode MS']\n",
        "\n",
        "    # Try to add a Telugu font directory if available\n",
        "    font_dirs = []\n",
        "    for font_dir in ['/usr/share/fonts/truetype/noto', '/usr/share/fonts/truetype/lohit',\n",
        "                    './fonts', '/kaggle/working/fonts']:\n",
        "        if os.path.exists(font_dir):\n",
        "            font_dirs.append(font_dir)\n",
        "\n",
        "    # Add custom fonts directory if exists\n",
        "    if len(font_dirs) > 0:\n",
        "        for font_dir in font_dirs:\n",
        "            fm.fontManager.addfont(font_dir)\n",
        "\n",
        "    # Set font family for matplotlib\n",
        "    plt.rcParams['font.family'] = 'sans-serif'\n",
        "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
        "\n",
        "    # Find a suitable Telugu font\n",
        "    found_font = False\n",
        "    for font in telugu_fonts:\n",
        "        if font in available_fonts:\n",
        "            plt.rcParams['font.sans-serif'] = [font] + plt.rcParams['font.sans-serif']\n",
        "            print(f\"Using font: {font} for Telugu characters\")\n",
        "            found_font = True\n",
        "            break\n",
        "\n",
        "    if not found_font:\n",
        "        plt.rcParams['font.sans-serif'] = ['DejaVu Sans'] + plt.rcParams['font.sans-serif']\n",
        "        print(\"Warning: No Telugu font found. Using DejaVu Sans, characters may not display correctly.\")\n",
        "\n",
        "    # Force matplotlib to use proper Unicode handling\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "    return found_font\n",
        "\n",
        "def generate_attention_heatmaps(model, test_loader, src_vocab, tgt_vocab,\n",
        "                               output_dir=\"./output\", filename=\"attention_heatmaps.png\",\n",
        "                               example_words=None):\n",
        "    \"\"\"\n",
        "    Generate attention heatmaps for visualization\n",
        "\n",
        "    Args:\n",
        "        model: Trained Seq2Seq model with attention\n",
        "        test_loader: DataLoader for test data\n",
        "        src_vocab: Source vocabulary\n",
        "        tgt_vocab: Target vocabulary\n",
        "        output_dir: Directory to save the visualization\n",
        "        filename: Filename for the output image\n",
        "        example_words: List of specific source words to visualize in order\n",
        "    \"\"\"\n",
        "    # Set up Telugu fonts\n",
        "    setup_telugu_fonts()\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
        "    output_file = Path(output_dir) / filename\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Make sure we have exactly the words we want to visualize\n",
        "    if not example_words:\n",
        "        example_words = [\"dhishaa\", \"bhaaratheeyulapai\", \"vahinchina\", \"ankela\",\n",
        "                         \"vishveshvarudini\", \"silva\", \"naats\", \"prayaanikulu\", \"deshamunaku\"]\n",
        "\n",
        "    # Store samples with attention weights\n",
        "    samples = []\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Process each example word in the order specified\n",
        "    for word in example_words:\n",
        "        # Encode the source word\n",
        "        src_ids = [src_vocab.sos_idx] + [src_vocab.char2idx.get(c, src_vocab.unk_idx) for c in word] + [src_vocab.eos_idx]\n",
        "        src = torch.tensor([src_ids], device=device)\n",
        "        src_lens = torch.tensor([len(src_ids)], device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get encoder outputs\n",
        "            enc_output, hidden = model.encoder(src, src_lens)\n",
        "\n",
        "            # Create mask for attention\n",
        "            mask = (src != model.pad_idx)\n",
        "\n",
        "            # Start decoding\n",
        "            input_token = torch.tensor([tgt_vocab.sos_idx], device=device)\n",
        "            predictions = []\n",
        "            attentions = []\n",
        "\n",
        "            # Decode one token at a time\n",
        "            for _ in range(50):\n",
        "                # Get embedding\n",
        "                emb = model.decoder.embedding(input_token.unsqueeze(0))\n",
        "\n",
        "                # Get hidden state for attention\n",
        "                if model.decoder.cell_type == 'LSTM':\n",
        "                    dec_h = hidden[0][-1]\n",
        "                else:\n",
        "                    dec_h = hidden[-1]\n",
        "\n",
        "                # Calculate attention\n",
        "                attn_weights = model.decoder.attention(dec_h, enc_output, mask)\n",
        "                attentions.append(tensor_to_numpy(attn_weights.squeeze()))\n",
        "\n",
        "                # Apply attention to encoder outputs\n",
        "                context = torch.bmm(attn_weights.unsqueeze(1), enc_output)\n",
        "\n",
        "                # Combine embedding and context\n",
        "                rnn_input = torch.cat((emb, context), dim=2)\n",
        "\n",
        "                # Pass through RNN\n",
        "                output, hidden = model.decoder.rnn(rnn_input, hidden)\n",
        "\n",
        "                # Generate output\n",
        "                output = output.squeeze(1)\n",
        "                emb = emb.squeeze(1)\n",
        "                context = context.squeeze(1)\n",
        "\n",
        "                logits = model.decoder.fc(torch.cat((output, context, emb), dim=1))\n",
        "\n",
        "                # Get next token\n",
        "                top1 = logits.argmax(1).item()\n",
        "                predictions.append(top1)\n",
        "\n",
        "                # Use as next input\n",
        "                input_token = torch.tensor([top1], device=device)\n",
        "\n",
        "                # Stop if EOS token\n",
        "                if top1 == tgt_vocab.eos_idx:\n",
        "                    break\n",
        "\n",
        "        # Convert predictions to string\n",
        "        pred_str = tgt_vocab.decode([p for p in predictions if p < len(tgt_vocab.idx2char) and p not in\n",
        "                                    [tgt_vocab.pad_idx, tgt_vocab.sos_idx, tgt_vocab.eos_idx]])\n",
        "\n",
        "        # Stack attention weights\n",
        "        attention_matrix = np.stack(attentions, axis=0)\n",
        "\n",
        "        # Only keep relevant part of source (remove <sos>, <eos>, and padding)\n",
        "        attention_matrix = attention_matrix[:, 1:len(word)+1]\n",
        "\n",
        "        # Add to samples\n",
        "        samples.append({\n",
        "            'src': word,\n",
        "            'pred': pred_str,\n",
        "            'attention': attention_matrix,\n",
        "            'src_tokens': list(word),\n",
        "            'pred_tokens': list(pred_str)\n",
        "        })\n",
        "\n",
        "    # Create the figure for the grid of heatmaps\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(20, 20), dpi=150)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # For each sample\n",
        "    for i, sample in enumerate(samples):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "\n",
        "        # Get the data for this heatmap\n",
        "        attn = sample['attention']\n",
        "        src_tokens = sample['src_tokens']\n",
        "        pred_tokens = sample['pred_tokens']\n",
        "\n",
        "        # Create the heatmap\n",
        "        sns.heatmap(\n",
        "            attn,\n",
        "            ax=axes[i],\n",
        "            cmap='Blues',  # Keep the blue colormap\n",
        "            xticklabels=src_tokens,\n",
        "            yticklabels=pred_tokens,  # Use Telugu characters\n",
        "            vmin=0.0,       # Consistent color scale\n",
        "            vmax=1.0,       # Consistent color scale\n",
        "            cbar_kws={'label': ''}  # Simplified colorbar\n",
        "        )\n",
        "\n",
        "        # Set title (Latin → Telugu)\n",
        "        axes[i].set_title(f\"{sample['src']} → {sample['pred']}\", fontsize=14)\n",
        "\n",
        "        # Set labels\n",
        "        axes[i].set_xlabel(\"Source (Latin)\", fontsize=12)\n",
        "        axes[i].set_ylabel(\"Target (Telugu)\", fontsize=12)\n",
        "\n",
        "        # Improve tick label size and rotation\n",
        "        axes[i].tick_params(axis='both', which='major', labelsize=12)\n",
        "        plt.setp(axes[i].get_xticklabels(), rotation=0)  # Keep x-labels horizontal\n",
        "        plt.setp(axes[i].get_yticklabels(), rotation=0)  # Keep y-labels horizontal\n",
        "\n",
        "        # Make sure the font used can display Telugu\n",
        "        for label in axes[i].get_yticklabels():\n",
        "            label.set_fontfamily('sans-serif')\n",
        "            label.set_fontsize(14)  # Larger font for Telugu\n",
        "\n",
        "    # Add overall title\n",
        "    fig.suptitle(\"Attention Heatmaps: Source to Target Character Alignment\", fontsize=18)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "    # Save the figure in high resolution\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Also save an SVG version which might handle Telugu better\n",
        "    svg_output = str(output_file).replace('.png', '.svg')\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i, sample in enumerate(samples):\n",
        "        if i >= 9:\n",
        "            break\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        sns.heatmap(\n",
        "            sample['attention'],\n",
        "            cmap='Blues',\n",
        "            xticklabels=sample['src_tokens'],\n",
        "            yticklabels=sample['pred_tokens'],\n",
        "            vmin=0.0, vmax=1.0,\n",
        "            cbar=True\n",
        "        )\n",
        "        plt.title(f\"{sample['src']} → {sample['pred']}\", fontsize=14)\n",
        "        plt.xlabel(\"Source (Latin)\", fontsize=12)\n",
        "        plt.ylabel(\"Target (Telugu)\", fontsize=12)\n",
        "    plt.suptitle(\"Attention Heatmaps: Source to Target Character Alignment\", fontsize=18)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.savefig(svg_output, format='svg', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Attention heatmaps saved to {output_file} and {svg_output}\")\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "uWf4lDMQmzjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Comparision table"
      ],
      "metadata": {
        "id": "lz4fNBEIIA0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "\n",
        "# Step 1: Load and preprocess datasets\n",
        "def load_predictions(file_path):\n",
        "    \"\"\"Load predictions and return a dictionary keyed by (source, target).\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return {(row['Source'], row['Target']): row['Predicted'] for _, row in df.iterrows()}\n",
        "\n",
        "# Step 2: Extract improved predictions\n",
        "def find_attention_advantages(attn_dict, plain_dict):\n",
        "    \"\"\"\n",
        "    Identify (source, target) pairs where the attention model was correct,\n",
        "    but the plain model was not.\n",
        "    \"\"\"\n",
        "    improved = []\n",
        "\n",
        "    for key in attn_dict:\n",
        "        src, tgt = key\n",
        "        pred_attn = attn_dict[key]\n",
        "        pred_plain = plain_dict.get(key, None)\n",
        "\n",
        "        if pred_attn == tgt and pred_plain and pred_plain != tgt:\n",
        "            improved.append({\n",
        "                \"Source Text\": src,\n",
        "                \"Expected Output\": tgt,\n",
        "                \"Prediction (Attention)\": pred_attn,\n",
        "                \"Prediction (Plain)\": pred_plain\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(improved)\n",
        "\n",
        "# Step 3: Save results and log to W&B\n",
        "def record_and_log(df, save_to, project_name=\"Transliteration-Attention-Study\"):\n",
        "    df.to_csv(save_to, index=False)\n",
        "    print(f\"[INFO] Saved attention improvement cases: {len(df)} → {save_to}\")\n",
        "\n",
        "    # Start a new W&B run and log results\n",
        "    run = wandb.init(project=project_name, name=\"Attention_Correction_Cases\", reinit=True)\n",
        "    wandb_table = wandb.Table(dataframe=df)\n",
        "    wandb.log({\"Attention Helps\": wandb_table})\n",
        "    wandb.finish()\n",
        "\n",
        "# Step 4: File setup\n",
        "root = Path(\"/kaggle/working/analysis_output\")\n",
        "root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "file_attention = root / \"attn_model_output.csv\"\n",
        "file_plain = root / \"plain_model_output.csv\"\n",
        "file_result = root / \"attention_corrections.csv\"\n",
        "\n",
        "# Step 5: Run the full pipeline\n",
        "attention_data = load_predictions(file_attention)\n",
        "plain_data = load_predictions(file_plain)\n",
        "\n",
        "result_df = find_attention_advantages(attention_data, plain_data)\n",
        "record_and_log(result_df, file_result)\n"
      ],
      "metadata": {
        "id": "ANmz3gAYJl18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tUp7rLY1JsoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_model(\n",
        "    cell_type='LSTM',\n",
        "    emb_size=256,\n",
        "    hidden_size=512,\n",
        "    enc_layers=2,\n",
        "    dec_layers=2,\n",
        "    dropout=0.2,\n",
        "    bidirectional=True,\n",
        "    use_attention=False,\n",
        "    batch_size=64,\n",
        "    learning_rate=0.001,\n",
        "    language='tel'\n",
        "):\n",
        "    \"\"\"\n",
        "    Constructs a Seq2Seq model instance with the given configuration.\n",
        "    This function matches the architecture and vocab setup used in the main training loop.\n",
        "    \"\"\"\n",
        "\n",
        "    # Vocabulary sizes (+2 for <sos>, <eos>)\n",
        "    input_vocab_size = len(eng_vocab) + 2\n",
        "    output_vocab_size = len(tel_vocab) + 2\n",
        "\n",
        "    # Initialize encoder\n",
        "    encoder = Encoder(\n",
        "        input_size=input_vocab_size,\n",
        "        embedding_size=emb_size,\n",
        "        enc_layers=enc_layers,\n",
        "        hidden_size=hidden_size,\n",
        "        cell_type=cell_type,\n",
        "        bi_directional_bit=bidirectional,\n",
        "        dropout=dropout,\n",
        "        batch_size=batch_size\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize decoder (choose attention or non-attention version)\n",
        "    if use_attention:\n",
        "        decoder = AttentionDecoder(\n",
        "            input_size=output_vocab_size,\n",
        "            embedding_size=emb_size,\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=output_vocab_size,\n",
        "            cell_type=cell_type,\n",
        "            dec_layers=dec_layers,\n",
        "            dropout=dropout,\n",
        "            bi_directional_bit=bidirectional\n",
        "        ).to(device)\n",
        "    else:\n",
        "        decoder = Decoder(\n",
        "            input_size=output_vocab_size,\n",
        "            embedding_size=emb_size,\n",
        "            hidden_size=hidden_size,\n",
        "            dec_layers=dec_layers,\n",
        "            dropout=dropout,\n",
        "            cell_type=cell_type,\n",
        "            output_size=output_vocab_size\n",
        "        ).to(device)\n",
        "\n",
        "    # Assemble the final model\n",
        "    model = Seq2Seq(\n",
        "        decoder=decoder,\n",
        "        encoder=encoder,\n",
        "        cell_type=cell_type,\n",
        "        bidirectional_bit=bidirectional,\n",
        "        encoder_layers=enc_layers,\n",
        "        decoder_layers=dec_layers\n",
        "    ).to(device)\n",
        "\n",
        "    return model\n",
        "model = construct_model()\n"
      ],
      "metadata": {
        "id": "x0x8lasBKifJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "heat map generation"
      ],
      "metadata": {
        "id": "MmU_ElggLPxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib import font_manager as fm\n",
        "import urllib.request\n",
        "\n",
        "# Set up directories\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "os.makedirs(\"telugu_fonts\", exist_ok=True)\n",
        "\n",
        "# Attempt to download and register Telugu font\n",
        "telugu_font_file = os.path.join(\"telugu_fonts\", \"Lohit-Telugu.ttf\")\n",
        "if not os.path.exists(telugu_font_file):\n",
        "    try:\n",
        "        urllib.request.urlretrieve(\n",
        "            \"https://releases.pagure.org/lohit/Lohit-Telugu.ttf\", telugu_font_file\n",
        "        )\n",
        "        print(f\"Font downloaded: {telugu_font_file}\")\n",
        "    except Exception as err:\n",
        "        print(f\"Font download failed: {err}\")\n",
        "        telugu_font_file = None\n",
        "\n",
        "if telugu_font_file and os.path.exists(telugu_font_file):\n",
        "    fm.fontManager.addfont(telugu_font_file)\n",
        "    telugu_font_prop = fm.FontProperties(fname=telugu_font_file)\n",
        "else:\n",
        "    telugu_font_prop = None\n",
        "\n",
        "def detach_tensor(t):\n",
        "    return t.detach().cpu().numpy() if t.requires_grad else t.cpu().numpy()\n",
        "\n",
        "def render_attention_grid(model, inputs, src_vocab, tgt_vocab, device, save_as=None):\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(22, 22))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Define a new dark mint-inspired colormap\n",
        "    color_steps = [\n",
        "        (0.02, 0.15, 0.12), (0.1, 0.3, 0.25), (0.2, 0.45, 0.38),\n",
        "        (0.3, 0.65, 0.52), (0.5, 0.8, 0.6), (0.7, 0.9, 0.75)\n",
        "    ]\n",
        "    custom_cmap = ListedColormap(color_steps, name=\"mint_flow\")\n",
        "\n",
        "    fig.patch.set_facecolor(\"#1A1A1A\")\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    for i, word in enumerate(inputs[:9]):\n",
        "        ax = axes[i]\n",
        "        model.eval()\n",
        "\n",
        "        input_ids = [src_vocab.sos_idx] + [src_vocab.char2idx.get(c, src_vocab.unk_idx) for c in word] + [src_vocab.eos_idx]\n",
        "        src_tensor = torch.tensor([input_ids], device=device)\n",
        "        src_len = torch.tensor([len(input_ids)], device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            enc_out, hidden = model.encoder(src_tensor, src_len)\n",
        "            mask = src_tensor != model.pad_idx\n",
        "            cur_token = torch.tensor([tgt_vocab.sos_idx], device=device)\n",
        "            decoded = []\n",
        "            attention_scores = []\n",
        "\n",
        "            for _ in range(50):\n",
        "                embedded = model.decoder.embedding(cur_token.unsqueeze(0))\n",
        "\n",
        "                if model.decoder.cell_type == 'LSTM':\n",
        "                    query_vector = hidden[0][-1]\n",
        "                else:\n",
        "                    query_vector = hidden[-1]\n",
        "\n",
        "                attn_score = model.decoder.attention(query_vector, enc_out, mask)\n",
        "                attention_scores.append(detach_tensor(attn_score.squeeze()))\n",
        "\n",
        "                ctx_vector = torch.bmm(attn_score.unsqueeze(1), enc_out)\n",
        "                rnn_input = torch.cat((embedded, ctx_vector), dim=2)\n",
        "                output, hidden = model.decoder.rnn(rnn_input, hidden)\n",
        "\n",
        "                output = output.squeeze(1)\n",
        "                ctx_vector = ctx_vector.squeeze(1)\n",
        "                embedded = embedded.squeeze(1)\n",
        "\n",
        "                logits = model.decoder.fc(torch.cat((output, ctx_vector, embedded), dim=1))\n",
        "                pred_idx = logits.argmax(1).item()\n",
        "                decoded.append(pred_idx)\n",
        "\n",
        "                if pred_idx == tgt_vocab.eos_idx:\n",
        "                    break\n",
        "\n",
        "                cur_token = torch.tensor([pred_idx], device=device)\n",
        "\n",
        "        input_chars = list(word)\n",
        "        filtered_preds = [idx for idx in decoded if idx not in {tgt_vocab.pad_idx, tgt_vocab.sos_idx, tgt_vocab.eos_idx}]\n",
        "        output_chars = [tgt_vocab.idx2char[idx] for idx in filtered_preds]\n",
        "\n",
        "        all_predictions.append((word, ''.join(output_chars)))\n",
        "\n",
        "        attn_matrix = np.stack(attention_scores)[:len(filtered_preds), 1:len(input_chars)+1]\n",
        "\n",
        "        heatmap = ax.imshow(attn_matrix, cmap=custom_cmap, aspect=\"auto\", vmin=0, vmax=1, interpolation='nearest')\n",
        "        ax.set_xticks(np.arange(len(input_chars)))\n",
        "        ax.set_xticklabels(input_chars, fontsize=14, fontweight=\"bold\", color='white')\n",
        "        ax.set_yticks(np.arange(len(output_chars)))\n",
        "        ax.set_facecolor('#101820')\n",
        "\n",
        "        if telugu_font_prop:\n",
        "            ax.set_yticklabels(output_chars, fontproperties=telugu_font_prop, fontsize=16, color='white')\n",
        "        else:\n",
        "            ax.set_yticklabels(output_chars, fontsize=16, color='white')\n",
        "\n",
        "        ax.set_title(word, fontsize=18, color='white')\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_edgecolor(\"#0ED2A7\")\n",
        "            spine.set_linewidth(2)\n",
        "\n",
        "    # Colorbar and figure title\n",
        "    cb_ax = fig.add_axes([0.92, 0.2, 0.015, 0.6])\n",
        "    colorbar = fig.colorbar(heatmap, cax=cb_ax)\n",
        "    colorbar.set_label(\"Attention Intensity\", fontsize=16, color='white')\n",
        "    colorbar.ax.tick_params(labelsize=12, colors='white')\n",
        "    colorbar.outline.set_edgecolor('white')\n",
        "\n",
        "    fig.suptitle(\"Character-Level Attention Visualization\", fontsize=24, color='white', y=0.97)\n",
        "    plt.subplots_adjust(left=0.05, right=0.9, top=0.94, bottom=0.06)\n",
        "\n",
        "    if save_as:\n",
        "        plt.savefig(save_as, dpi=300, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
        "        print(f\"[✓] Attention heatmap saved to: {save_as}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print word-level outputs\n",
        "    print(\"\\nGenerated Telugu Words:\")\n",
        "    for src, tgt in all_predictions:\n",
        "        print(f\"{src} → {tgt}\")\n",
        "\n",
        "# Sample test inputs (new unique examples)\n",
        "test_examples = [\n",
        "    \"kanthari\", \"manohara\", \"narayana\",\n",
        "    \"charitra\", \"bhavanam\", \"sangathi\",\n",
        "    \"suraksha\", \"bhakti\", \"rajadhani\"\n",
        "]\n",
        "\n",
        "# Call function to generate visual heatmaps\n",
        "output_path = os.path.join(\"outputs\", \"heatmaps_mint_theme.png\")\n",
        "render_attention_grid(model, test_examples, src_vocab, tgt_vocab, device, output_path)\n"
      ],
      "metadata": {
        "id": "kE8ANlRWL_vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATTENTION VISUALIZATION"
      ],
      "metadata": {
        "id": "5Z7QYbg5MB5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import base64\n",
        "import torch\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from matplotlib import font_manager as fm\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from IPython.display import HTML, display\n",
        "import imageio.v3 as iio\n",
        "\n",
        "# Create necessary folders\n",
        "Path(\"visuals/frames\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"visuals/fonts\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download Telugu font if missing\n",
        "telugu_font_path = \"visuals/fonts/Lohit-Telugu.ttf\"\n",
        "if not os.path.exists(telugu_font_path):\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://releases.pagure.org/lohit/Lohit-Telugu.ttf\", telugu_font_path\n",
        "    )\n",
        "\n",
        "fm.fontManager.addfont(telugu_font_path)\n",
        "telugu_font = fm.FontProperties(fname=telugu_font_path)\n",
        "\n",
        "def extract_attention(model, word, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "    input_ids = [src_vocab.sos_idx] + [src_vocab.char2idx.get(c, src_vocab.unk_idx) for c in word] + [src_vocab.eos_idx]\n",
        "    src_tensor = torch.tensor([input_ids], device=device)\n",
        "    src_len = torch.tensor([len(input_ids)], device=device)\n",
        "\n",
        "    predictions = []\n",
        "    attn_weights = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_out, hidden = model.encoder(src_tensor, src_len)\n",
        "        mask = src_tensor != model.pad_idx\n",
        "        token = torch.tensor([tgt_vocab.sos_idx], device=device)\n",
        "\n",
        "        for _ in range(40):\n",
        "            emb = model.decoder.embedding(token.unsqueeze(0))\n",
        "            query = hidden[0][-1] if model.decoder.cell_type == 'LSTM' else hidden[-1]\n",
        "            attn = model.decoder.attention(query, enc_out, mask)\n",
        "            attn_weights.append(attn.squeeze().cpu().numpy())\n",
        "\n",
        "            ctx = torch.bmm(attn.unsqueeze(1), enc_out)\n",
        "            rnn_input = torch.cat((emb, ctx), dim=2)\n",
        "            output, hidden = model.decoder.rnn(rnn_input, hidden)\n",
        "\n",
        "            logits = model.decoder.fc(torch.cat((output.squeeze(1), ctx.squeeze(1), emb.squeeze(1)), dim=1))\n",
        "            pred = logits.argmax(1).item()\n",
        "            if pred == tgt_vocab.eos_idx:\n",
        "                break\n",
        "            predictions.append(pred)\n",
        "            token = torch.tensor([pred], device=device)\n",
        "\n",
        "    pred_chars = [tgt_vocab.idx2char[i] for i in predictions if i < len(tgt_vocab.idx2char)]\n",
        "    return list(word), pred_chars, attn_weights\n",
        "\n",
        "def draw_frame(src_chars, tgt_chars, attn_matrix, idx, cmap):\n",
        "    fig, ax = plt.subplots(figsize=(10, 2.5))\n",
        "    fig.patch.set_facecolor('#1a1a1a')\n",
        "    ax.set_facecolor('#000000')\n",
        "\n",
        "    attn_array = np.stack(attn_matrix)[:len(tgt_chars), :len(src_chars)]\n",
        "    ax.imshow(attn_array, cmap=cmap, vmin=0, vmax=1, aspect='auto')\n",
        "\n",
        "    ax.set_xticks(np.arange(len(src_chars)))\n",
        "    ax.set_yticks(np.arange(len(tgt_chars)))\n",
        "    ax.set_xticklabels(src_chars, fontsize=12, color='white')\n",
        "    ax.set_yticklabels(tgt_chars, fontsize=12, color='white', fontproperties=telugu_font)\n",
        "\n",
        "    ax.set_title(f\"Generating Step {idx+1}\", fontsize=14, color='cyan')\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_color('white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def create_attention_gif(model, words, src_vocab, tgt_vocab, device, out_file=\"visuals/animated.gif\", fps=2):\n",
        "    frames = []\n",
        "    cmap = LinearSegmentedColormap.from_list(\"attn_cmap\", [(0.2, 0.2, 0.6), (0.4, 0.8, 0.8), (0.9, 1.0, 1.0)])\n",
        "\n",
        "    for word in words:\n",
        "        src_chars, tgt_chars, attn_matrix = extract_attention(model, word, src_vocab, tgt_vocab, device)\n",
        "        for i in range(len(tgt_chars)):\n",
        "            fig = draw_frame(src_chars, tgt_chars, attn_matrix, i, cmap)\n",
        "            buf = io.BytesIO()\n",
        "            fig.savefig(buf, format='png', bbox_inches='tight', facecolor=fig.get_facecolor())\n",
        "            buf.seek(0)\n",
        "            frames.append(np.array(Image.open(buf)))\n",
        "            plt.close(fig)\n",
        "\n",
        "    iio.imwrite(out_file, frames, format='GIF', duration=1/fps)\n",
        "    print(f\"[✓] Saved animation to {out_file}\")\n",
        "    return out_file\n",
        "\n",
        "def display_inline(gif_path):\n",
        "    with open(gif_path, \"rb\") as f:\n",
        "        data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "    html = f'<img src=\"data:image/gif;base64,{data}\" style=\"max-width: 100%\" loop=\"infinite\">'\n",
        "    display(HTML(html))\n",
        "\n",
        "# === Usage ===\n",
        "# example_words = [\"satyanarayana\", \"tirupathi\", \"vijayanagara\"]\n",
        "# gif_file = create_attention_gif(model, example_words, src_vocab, tgt_vocab, device)\n",
        "# display_inline(gif_file)\n"
      ],
      "metadata": {
        "id": "kDwpY4mtQi-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}